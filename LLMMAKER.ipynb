{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWEpBA4OZqDqvnWfCZ9A4O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pokemonmaster67/Magical-LLM/blob/main/LLMMAKER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s35lrt1VsD3N",
        "outputId": "87d9c4fb-8154-4236-ab34-9547ca5d9763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LLM Maker 2.0 Menu:\n",
            "1. Create/Load Model\n",
            "2. Train Model\n",
            "3. Fine-tune Model\n",
            "4. Evaluate Model\n",
            "5. Generate Text\n",
            "6. Save Model\n",
            "7. Load Model\n",
            "8. Adjust Hyperparameters\n",
            "9. Visualize Attention\n",
            "10. Exit\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer\n",
        "import argparse\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "\n",
        "class LLMMaker:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.config = None\n",
        "        self.model_name = None\n",
        "\n",
        "    def create_model(self, from_scratch=True, base_model='gpt2', vocab_size=50257, n_layer=12, n_head=12, n_embd=768, model_name=\"MyModel\"):\n",
        "        self.model_name = model_name\n",
        "        if from_scratch:\n",
        "            if n_embd % n_head != 0:\n",
        "                n_embd = (n_embd // n_head) * n_head\n",
        "                print(f\"Adjusted n_embd to {n_embd} to ensure divisibility by n_head\")\n",
        "\n",
        "            self.config = GPT2Config(\n",
        "                vocab_size=vocab_size,\n",
        "                n_layer=n_layer,\n",
        "                n_head=n_head,\n",
        "                n_embd=n_embd\n",
        "            )\n",
        "            try:\n",
        "                self.model = GPT2LMHeadModel(self.config)\n",
        "                self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "                print(f\"Model '{self.model_name}' created from scratch successfully!\")\n",
        "            except ValueError as e:\n",
        "                print(f\"Error creating model: {e}\")\n",
        "                print(\"Please adjust the parameters and try again.\")\n",
        "                return\n",
        "        else:\n",
        "            try:\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "                self.config = self.model.config\n",
        "                print(f\"Model '{self.model_name}' loaded from {base_model} for fine-tuning!\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model: {e}\")\n",
        "                print(\"Please check the base model name and try again.\")\n",
        "                return\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.model.config.pad_token_id = self.model.config.eos_token_id\n",
        "            print(\"Added padding token to the tokenizer.\")\n",
        "\n",
        "    def train_model(self, train_data, epochs=1, learning_rate=5e-5):\n",
        "        if self.model is None:\n",
        "            print(\"Please create a model first!\")\n",
        "            return\n",
        "\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for input_text, target_text in train_data:\n",
        "                # Encode input and target\n",
        "                inputs = self.tokenizer(input_text, return_tensors='pt', truncation=True, padding=True)\n",
        "                targets = self.tokenizer(target_text, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "                # Ensure input and target have the same sequence length\n",
        "                max_length = max(inputs['input_ids'].size(1), targets['input_ids'].size(1))\n",
        "                inputs = self.tokenizer(input_text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "                targets = self.tokenizer(target_text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "\n",
        "                input_ids = inputs['input_ids']\n",
        "                target_ids = targets['input_ids']\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(input_ids, labels=target_ids)\n",
        "                loss = outputs.loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_data)}\")\n",
        "\n",
        "    def fine_tune(self, fine_tune_data, epochs=1, learning_rate=1e-5):\n",
        "        self.train_model(fine_tune_data, epochs, learning_rate)\n",
        "\n",
        "    def evaluate_model(self, eval_data):\n",
        "        if self.model is None:\n",
        "            print(\"Please create a model first!\")\n",
        "            return\n",
        "\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for input_text, target_text in eval_data:\n",
        "                # Encode input and target\n",
        "                inputs = self.tokenizer(input_text, return_tensors='pt', truncation=True, padding=True)\n",
        "                targets = self.tokenizer(target_text, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "                # Ensure input and target have the same sequence length\n",
        "                max_length = max(inputs['input_ids'].size(1), targets['input_ids'].size(1))\n",
        "                inputs = self.tokenizer(input_text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "                targets = self.tokenizer(target_text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "\n",
        "                input_ids = inputs['input_ids']\n",
        "                target_ids = targets['input_ids']\n",
        "\n",
        "                outputs = self.model(input_ids, labels=target_ids)\n",
        "                total_loss += outputs.loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(eval_data)\n",
        "        perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "        print(f\"Evaluation Loss: {avg_loss}\")\n",
        "        print(f\"Perplexity: {perplexity.item()}\")\n",
        "\n",
        "    def generate_text(self, prompt, max_length=100):\n",
        "        if self.model is None:\n",
        "            print(\"Please create a model first!\")\n",
        "            return\n",
        "\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "        output = self.model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        print(f\"Generated text: {generated_text}\")\n",
        "\n",
        "    def save_model(self, path):\n",
        "        if self.model is None:\n",
        "            print(\"Please create a model first!\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "            torch.save(self.model.state_dict(), os.path.join(path, f\"{self.model_name}.pth\"))\n",
        "            self.tokenizer.save_pretrained(path)\n",
        "            self.config.to_json_file(os.path.join(path, \"config.json\"))\n",
        "            print(f\"Model '{self.model_name}' saved successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while saving the model: {str(e)}\")\n",
        "\n",
        "    def load_model(self, path):\n",
        "        try:\n",
        "            config_path = os.path.join(path, \"config.json\")\n",
        "            self.config = GPT2Config.from_json_file(config_path)\n",
        "            self.model = GPT2LMHeadModel(self.config)\n",
        "            self.model.load_state_dict(torch.load(os.path.join(path, f\"{self.model_name}.pth\")))\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(path)\n",
        "            print(f\"Model '{self.model_name}' loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while loading the model: {str(e)}\")\n",
        "\n",
        "    def adjust_hyperparameters(self, learning_rate=None, batch_size=None, dropout=None):\n",
        "        if self.model is None:\n",
        "            print(\"Please create a model first!\")\n",
        "            return\n",
        "\n",
        "        if learning_rate:\n",
        "            for param_group in self.model.optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "            print(f\"Learning rate adjusted to {learning_rate}\")\n",
        "\n",
        "        if batch_size:\n",
        "            print(f\"Batch size set to {batch_size}\")\n",
        "\n",
        "        if dropout is not None:\n",
        "            for module in self.model.modules():\n",
        "                if isinstance(module, torch.nn.Dropout):\n",
        "                    module.p = dropout\n",
        "            print(f\"Dropout rate adjusted to {dropout}\")\n",
        "\n",
        "    def visualize_attention(self, text):\n",
        "        if self.model is None:\n",
        "            print(\"Please create a model first!\")\n",
        "            return\n",
        "\n",
        "        inputs = self.tokenizer(text, return_tensors='pt')\n",
        "        outputs = self.model(**inputs)\n",
        "        attention = outputs.attentions[-1].mean(dim=1).mean(dim=1).detach().numpy()\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(attention, annot=True, cmap='YlGnBu')\n",
        "        plt.title('Attention Visualization')\n",
        "        plt.xlabel('Token Position')\n",
        "        plt.ylabel('Token Position')\n",
        "        plt.show()\n",
        "\n",
        "def get_training_data_from_terminal():\n",
        "    training_data = []\n",
        "    print(\"Enter input-output pairs for training. Type 'done' when finished.\")\n",
        "    while True:\n",
        "        input_text = input(\"Input: \")\n",
        "        if input_text.lower() == 'done':\n",
        "            break\n",
        "        output_text = input(\"Output: \")\n",
        "        training_data.append((input_text, output_text))\n",
        "    return training_data\n",
        "\n",
        "def main():\n",
        "    llm_maker = LLMMaker()\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nLLM Maker 2.0 Menu:\")\n",
        "        print(\"1. Create/Load Model\")\n",
        "        print(\"2. Train Model\")\n",
        "        print(\"3. Fine-tune Model\")\n",
        "        print(\"4. Evaluate Model\")\n",
        "        print(\"5. Generate Text\")\n",
        "        print(\"6. Save Model\")\n",
        "        print(\"7. Load Model\")\n",
        "        print(\"8. Adjust Hyperparameters\")\n",
        "        print(\"9. Visualize Attention\")\n",
        "        print(\"10. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-10): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            create_choice = input(\"Do you want to create a model from scratch? (yes/no): \").lower()\n",
        "            model_name = input(\"Enter a name for your model: \")\n",
        "            if create_choice == 'yes':\n",
        "                vocab_size = int(input(\"Enter vocabulary size (default 50257): \") or 50257)\n",
        "                n_layer = int(input(\"Enter number of layers (default 12): \") or 12)\n",
        "                n_head = int(input(\"Enter number of attention heads (default 12): \") or 12)\n",
        "                n_embd = int(input(\"Enter embedding dimension (default 768): \") or 768)\n",
        "                llm_maker.create_model(from_scratch=True, vocab_size=vocab_size, n_layer=n_layer, n_head=n_head, n_embd=n_embd, model_name=model_name)\n",
        "            else:\n",
        "                base_model = input(\"Enter the name of the pre-trained model to fine-tune (e.g., 'gpt2', 'gpt2-medium'): \")\n",
        "                llm_maker.create_model(from_scratch=False, base_model=base_model, model_name=model_name)\n",
        "\n",
        "        elif choice == '2':\n",
        "            train_data = get_training_data_from_terminal()\n",
        "            epochs = int(input(\"Enter number of epochs: \"))\n",
        "            learning_rate = float(input(\"Enter learning rate: \"))\n",
        "            llm_maker.train_model(train_data, epochs, learning_rate)\n",
        "\n",
        "        elif choice == '3':\n",
        "            fine_tune_data = get_training_data_from_terminal()\n",
        "            epochs = int(input(\"Enter number of epochs: \"))\n",
        "            learning_rate = float(input(\"Enter learning rate: \"))\n",
        "            llm_maker.fine_tune(fine_tune_data, epochs, learning_rate)\n",
        "\n",
        "        elif choice == '4':\n",
        "            eval_data = get_training_data_from_terminal()\n",
        "            llm_maker.evaluate_model(eval_data)\n",
        "\n",
        "        elif choice == '5':\n",
        "            prompt = input(\"Enter a prompt for text generation: \")\n",
        "            max_length = int(input(\"Enter maximum length for generated text: \"))\n",
        "            llm_maker.generate_text(prompt, max_length)\n",
        "\n",
        "        elif choice == '6':\n",
        "            path = input(\"Enter path to save the model: \")\n",
        "            llm_maker.save_model(path)\n",
        "\n",
        "        elif choice == '7':\n",
        "            path = input(\"Enter path to load the model: \")\n",
        "            model_name = input(\"Enter the name of the model to load: \")\n",
        "            llm_maker.model_name = model_name\n",
        "            llm_maker.load_model(path)\n",
        "\n",
        "        elif choice == '8':\n",
        "            lr = float(input(\"Enter new learning rate (or press Enter to skip): \") or 0)\n",
        "            bs = int(input(\"Enter new batch size (or press Enter to skip): \") or 0)\n",
        "            dr = float(input(\"Enter new dropout rate (or press Enter to skip): \") or -1)\n",
        "            llm_maker.adjust_hyperparameters(lr or None, bs or None, dr if dr >= 0 else None)\n",
        "\n",
        "        elif choice == '9':\n",
        "            text = input(\"Enter text for attention visualization: \")\n",
        "            llm_maker.visualize_attention(text)\n",
        "\n",
        "        elif choice == '10':\n",
        "            print(\"Thank you for using LLM Maker 2.0!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}